<h1 align="center">
  DELTAv2: Accelerating Dense 3D Tracking
</h1>

This is the official GitHub repository of the paper:

**[DELTAv2: Accelerating Dense 3D Tracking](https://snap-research.github.io/DELTA/)**
</br>
[Tuan Duc Ngo](https://ngoductuanlhp.github.io/),
[Ashkan Mirzaei](https://ashmrz.github.io/),
[Gordon Qian](https://guochengqian.github.io/),
[Hanwen Liang](https://scholar.google.com/citations?user=mrOHvI8AAAAJ&hl=en),
[Chuang Gan](https://people.csail.mit.edu/ganchuang/),
[Evangelos Kalogerakis](https://kalo-ai.github.io/),
[Peter Wonka](https://peterwonka.net/),
[Chaoyang Wang](https://mightychaos.github.io/),
</br>

### [Project Page](https://snap-research.github.io/DELTA/) | [Arxiv](https://arxiv.org/abs/2410.24211) | [Paper](https://snap-research.github.io/DELTA/files/paper.pdf) | [BibTeX](#citing-delta)


<img width="1100" src="./assets/teaser.png" />

**DELTAv2** accelerates **dense 3D tracking** by a factor of **5** compared to DELTA, while achieving **comparable performance**.

## TODO
- [] Release the code

## Getting Started

### Installation

1. Clone DELTAv2.
```bash
git clone --recursive https://github.com/snap-research/DenseTrack3Dv2
cd DenseTrack3Dv2
## if you have already cloned DenseTrack3D:
# git submodule update --init --recursive
```

2. Create the environment.
```bash
conda create -n densetrack3d python=3.10 cmake=3.14.0 -y # we recommend using python<=3.10
conda activate densetrack3d 
conda install pytorch torchvision pytorch-cuda=12.1 -c pytorch -c nvidia -y  # use the correct version of cuda for your system

pip install pip==24.0 # downgrade pip to install pytorch_lightning==1.6.0
pip install -r requirements.txt
conda install ffmpeg -c conda-forge # to write .mp4 video

pip install -U "ray[default]" # for parallel processing
pip install viser # for visualize 3D trajectories
```

3. Install `Unidepth`.
```bash
pip install ninja
pip install -v -U git+https://github.com/facebookresearch/xformers.git@v0.0.24 # Unidepth requires xformers==0.0.2
pip install "git+https://github.com/facebookresearch/pytorch3d.git@stable" # or go to submodules/UniDepth/unidepth/ops/knn/src then 'bash compile.sh'
```

4. [Optional] Install `viser` and `open3d` for 3D visualization.

```bash
pip install viser
pip install open3d
```

5. [Optional] Install dependencies to generate training data with [Kubric](https://github.com/google-research/kubric).

```bash
pip install bpy==3.4.0
pip install pybullet
pip install OpenEXR
pip install tensorflow tensorflow-datasets>=4.1.0 tensorflow-graphics

cd data/kubric/
pip install -e .
cd ../..
```

6. [Optional] Install dependencies to run camera pose estimation.

```bash
pip install roma trimesh

cd submodules/Grounded-SAM-2
pip install -e .
pip install --no-build-isolation -e grounding_dino

```


### Download Checkpoints

The pretrained checkpoints can be downloaded on [Google Drive](https://drive.google.com/file/d/1Qa9YFAjBFIzrrHHWf8NZMln5YkLfw4Qa/view?usp=sharing).


Run the following commands to download:
```bash
# download the weights
mkdir -p ./checkpoints/
gdown --fuzzy https://drive.google.com/file/d/1Qa9YFAjBFIzrrHHWf8NZMln5YkLfw4Qa/view?usp=sharing -O ./checkpoints/ # 3D ckpt

```


### Inference

1. We currently support 4 different tracking modes, including **Dense 3D Tracking**, **Sparse 3D Tracking**, **Dense 2D Tracking**, and **Sparse 2D Tracking**. We include 3 sample videos (`car-roundabout`, `rollerblade` from DAVIS, and `yellow-duck` generated by SORA) in this repo.

- **Dense 3D Tracking**: This is the main contribution of our work, where the model takes an RGB-D video (the videodepth can be obtained by an off-the-shelf depth estimator) and outputs a dense 3D trajectory map. To run the inference code, you can use the following command:

  ```bash
  python3 demo.py --ckpt checkpoints/densetrack3d.pth --video_path demo_data/yellow-duck --output_path results/demo # run with Unidepth

  # or
  python3 demo.py --ckpt logdirs/densetrack3d/model_densetrack3d_pyr_002112.pth --video_path demo_data/yellow-duck --output_path results/pyramid --use_depthcrafter # run with DepthCrafter
  ```

  By default, densely tracking a video of ~100 frames requires ~40GB of GPU memory. To reduce memory consumption, we can use a larger upsample factor (e.g., 8x) and enable fp16 inference, which reduces the requirement to ~20GB of GPU memory:

  ```bash
  python3 demo.py --upsample_factor 8 --use_fp16 --ckpt checkpoints/densetrack3d.pth --video_path demo_data/yellow-duck --output_path results/demo
  ```


- **Sparse 3D Tracking**: We also support sparse 3D point tracking (similar to [SceneTracker](https://github.com/wwsource/SceneTracker) and [SpaTracker](https://github.com/henry123-boy/SpaTracker)), where users can specify which points to track or the model will track a sparse grid of points by default.

  ```bash
  python3 demo_sparse.py --ckpt checkpoints/densetrack3d.pth --video_path demo_data/yellow-duck --output_path results/demo # run with Unidepth

  # or
  python3 demo_sparse.py --ckpt checkpoints/densetrack3d.pth --video_path demo_data/yellow-duck --output_path results/demo --use_depthcrafter # run with DepthCrafter
  ```

- **Dense 2D Tracking**: This mode is similar to [DOT](https://github.com/16lemoing/dot), where the model only takes an RGB video (no depth input) and outputs the dense 2D coords map (UV map).

  ```bash
  python3 demo_2d.py --ckpt checkpoints/densetrack2d.pth --video_path demo_data/yellow-duck --output_path results/demo
  ```

- **Sparse 2D Tracking**: This mode is similar to [CoTracker](https://github.com/facebookresearch/co-tracker), where the model only takes an RGB video as input, then users can specify which points to track or the model will track a sparse grid of points by default. The output is a set of 2D trajectories.

  ```bash
  python3 demo_2d_sparse.py --ckpt checkpoints/densetrack2d.pth --video_path demo_data/yellow-duck --output_path results/demo
  ```

2. [Optional] Visualize the dense 3D tracks with `viser`:

```bash
python3 visualizer/vis_densetrack3d.py --filepath results/demo/yellow-duck/dense_3d_track.pkl
```

3. [Optional] Visualize the dense 3D tracks with `open3d` (GUI required). To highlight the trajectories of the foreground object, we provide a binary foreground mask for the first frame of the video (the starting frame for dense tracking), which can be obtained with SAM.

```bash
# first run with mode=choose_viewpoint, a 3D GUI will pop-up and you can select the proper viewpoint to capture. Press "S" to save the viewpoint and exit.
python3 visualizer/vis_open3d.py --filepath results/yellow-duck/dense_3d_track.pkl --fg_mask_path demo_data/yellow-duck/yellow-duck_mask.png --video_name yellow-duck --mode choose_viewpoint

# Then run with mode=capture to start rendering 2D video of dense tracking
python3 visualizer/vis_open3d.py --filepath results/yellow-duck/dense_3d_track.pkl --fg_mask_path demo_data/yellow-duck/yellow-duck_mask.png --video_name yellow-duck --mode capture
```

### Prepare training & evaluation data
Please follow the instructions [here](data/DATA_PREPARATION.md) to prepare the training & evaluation data

### Training

1. Pretrain dense 2D tracking model

```bash
bash scripts/train/pretrain_2d.sh
```

2. Train dense 3D tracking model

```bash
bash scripts/train/train.sh
```

### Evaluation 

1. Evaluate sparse 3D tracking on the [TAPVid3D Benchmark](https://tapvid3d.github.io/)

```bash
# Note: replace TAPVID3D_DIR with the real path to tapvid3d dataset
python3 scripts/eval/eval_3d.py
```

2. Evaluate dense 2D tracking on the [CVO Benchmark](https://16lemoing.github.io/dot/)

```bash
# Note: replace CVO_DIR with the real path to CVO dataset
python3 scripts/eval/eval_flow2d.py
```

3. Evaluate sparse 2D tracking on the [TAPVid2D Benchmark](https://tapvid.github.io/)

```bash
# Note: replace TAPVID2D_DIR with the real path to tapvid2d dataset
python3 scripts/eval/eval_2d.py
```

<!-- ## Citing DELTA

If you find our repository useful, please consider giving it a star â­ and citing our paper in your work:

```bibtex
@article{ngo2024delta,
  author    = {Ngo, Tuan Duc and Zhuang, Peiye and Gan, Chuang and Kalogerakis, Evangelos and Tulyakov, Sergey and Lee, Hsin-Ying and Wang, Chaoyang},
  title     = {DELTAv2: Dense Efficient Long-range 3D Tracking for Any video},
  journal   = {arXiv preprint arXiv:2410.24211},
  year      = {2024}
} -->
```

## Acknowledgements

Our code is based on [DELTA](https://snap-research.github.io/DELTA/), [Monst3r](https://monst3r-project.github.io/), the training data generation is based on [Kubric](https://github.com/google-research/kubric), and our visualization code is based on [Viser](https://viser.studio/main/) and [Open3D](https://www.open3d.org/). We thank the authors for their excellent work!