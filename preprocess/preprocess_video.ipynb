{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "os.sys.path.append(os.path.abspath(os.path.join(BASE_DIR, \"submodules\", \"UniDepth\")))\n",
    "os.sys.path.append(os.path.abspath(os.path.join(BASE_DIR, \"submodules\", \"DepthCrafter\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import io\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import mediapy as media\n",
    "import IPython\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "# print()\n",
    "\n",
    "from unidepth.models import UniDepthV1, UniDepthV2\n",
    "from unidepth.utils import colorize, image_grid\n",
    "\n",
    "from diffusers.training_utils import set_seed\n",
    "\n",
    "try:\n",
    "    from depthcrafter.depth_crafter_ppl import DepthCrafterPipeline\n",
    "    from depthcrafter.unet import DiffusersUNetSpatioTemporalConditionModelDepthCrafter\n",
    "    from depthcrafter.utils import vis_sequence_depth, save_video, read_video_frames, read_folder_frames, read_video\n",
    "    print(\"DepthCrafter is available\")\n",
    "    use_depthcrafter = True\n",
    "except:\n",
    "    print(\"DepthCrafter is not available\")\n",
    "    use_depthcrafter = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "unidepth_model = UniDepthV2.from_pretrained(f\"lpiccinelli/unidepth-v2-vitl14\")\n",
    "unidepth_model = unidepth_model.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_depthcrafter:\n",
    "\n",
    "    unet = DiffusersUNetSpatioTemporalConditionModelDepthCrafter.from_pretrained(\n",
    "        \"tencent/DepthCrafter\",\n",
    "        low_cpu_mem_usage=True,\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    # load weights of other components from the provided checkpoint\n",
    "    depth_crafter_pipe = DepthCrafterPipeline.from_pretrained(\n",
    "        \"stabilityai/stable-video-diffusion-img2vid-xt\",\n",
    "        unet=unet,\n",
    "        torch_dtype=torch.float16,\n",
    "        variant=\"fp16\",\n",
    "    )\n",
    "\n",
    "    depth_crafter_pipe.to(\"cuda\")\n",
    "    # enable attention slicing and xformers memory efficient attention\n",
    "    try:\n",
    "        depth_crafter_pipe.enable_xformers_memory_efficient_attention()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Xformers is not enabled\")\n",
    "    depth_crafter_pipe.enable_attention_slicing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def predict_unidepth(video, model):\n",
    "    video_torch = torch.from_numpy(video).permute(0,3,1,2).to(device)\n",
    "\n",
    "    depth_pred = []\n",
    "    chunks = torch.split(video_torch, 32, dim=0)\n",
    "    for chunk in chunks:\n",
    "        predictions = model.infer(chunk)\n",
    "        depth_pred_ = predictions[\"depth\"].squeeze(1).cpu().numpy()\n",
    "        depth_pred.append(depth_pred_)\n",
    "    depth_pred = np.concatenate(depth_pred, axis=0)\n",
    "\n",
    "    return depth_pred\n",
    "\n",
    "@torch.inference_mode()\n",
    "def predict_depthcrafter(video, pipe):\n",
    "    frames, ori_h, ori_w = read_video(\n",
    "        video, max_res=1024\n",
    "    )\n",
    "    res = pipe(\n",
    "        frames,\n",
    "        height=frames.shape[1],\n",
    "        width=frames.shape[2],\n",
    "        output_type=\"np\",\n",
    "        guidance_scale=1.2,\n",
    "        num_inference_steps=25,\n",
    "        window_size=110,\n",
    "        overlap=25,\n",
    "        track_time=False,\n",
    "    ).frames[0]\n",
    "\n",
    "    # convert the three-channel output to a single channel depth map\n",
    "    res = res.sum(-1) / res.shape[-1]\n",
    "    # normalize the depth map to [0, 1] across the whole video\n",
    "    res = (res - res.min()) / (res.max() - res.min())\n",
    "    \n",
    "    res = F.interpolate(torch.from_numpy(res[:, None]), (ori_h, ori_w), mode='nearest').squeeze(1).numpy()\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vid_names = [\"lady-running\"]\n",
    "\n",
    "\n",
    "\n",
    "for vid_name in vid_names:\n",
    "    if os.path.exists(os.path.join(\"demo_data\", vid_name, \"color.mp4\")):\n",
    "        video = media.read_video(os.path.join(\"demo_data\", vid_name, \"color.mp4\"))\n",
    "    elif os.path.isdir(os.path.join(\"demo_data\", vid_name, \"color\")):\n",
    "        rgb_folder = os.path.join(\"demo_data/\", vid_name, \"color\")\n",
    "        rgb_frames = sorted([f for f in os.listdir(rgb_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "        video = []\n",
    "\n",
    "        for rgb_frame in rgb_frames:\n",
    "            img = cv2.imread(os.path.join(rgb_folder, rgb_frame))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            video.append(img)\n",
    "            \n",
    "        video = np.stack(video)\n",
    "    else:\n",
    "        raise ValueError(\"No video found\")\n",
    "    \n",
    "    print(\"Run Unidepth\")\n",
    "    depth_pred = predict_unidepth(video, unidepth_model)\n",
    "    np.save(os.path.join(\"demo_data\", vid_name, \"depth_pred.npy\"), depth_pred)\n",
    "\n",
    "    if use_depthcrafter: \n",
    "        print(\"Run DepthCrafter\")   \n",
    "        disp_pred = predict_depthcrafter(video, depth_crafter_pipe)\n",
    "        np.save(os.path.join(\"demo_data\", vid_name, \"depth_depthcrafter.npy\"), disp_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "track",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
